# llm_wrapper.py

from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

# Initialize the Phi-3.8b model from Ollama
llm = ChatOllama(model="phi3:3.8b")

def get_llm_response(prompt: str, word_limit: int = None) -> str:
    """
    Generate a response from Phi-3 via Ollama with strong guidance on word count.

    Args:
        prompt (str): The user query (can include context).
        word_limit (int, optional): Desired response length in words.

    Returns:
        str: Response generated by the model.
    """
    try:
        # Stronger guidance through system prompt
        if word_limit:
            system_instruction = (
                f"You are a helpful assistant. Provide a detailed answer of around {word_limit} words. "
                f"If the question is complex, explain all relevant aspects thoroughly to meet the word count."
            )
        else:
            system_instruction = "You are a helpful assistant. Answer the question clearly and concisely."

        # Send chat-style prompt
        response = llm.invoke([
            SystemMessage(content=system_instruction),
            HumanMessage(content=prompt)
        ])
        return response.content

    except Exception as e:
        return f"⚠️ Error generating response: {e}"
