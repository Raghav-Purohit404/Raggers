import os
import time
import hashlib
import pickle
from pathlib import Path
from typing import List
from langchain.schema import Document
from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredFileLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pptx import Presentation
from bs4 import BeautifulSoup
import requests

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
HASH_STORE_PATH = "indexed_hashes.pkl"
SUPPORTED_EXTENSIONS = [".pdf", ".txt", ".md", ".csv", ".docx", ".ppt", ".pptx"]
MIN_TOKENS = 20  # üßπ Filter out too short chunks
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

# Load or init hash memory
if os.path.exists(HASH_STORE_PATH):
    with open(HASH_STORE_PATH, "rb") as f:
        indexed_hashes = pickle.load(f)
else:
    indexed_hashes = set()

# üîë Hash utility
def hash_content(text: str) -> str:
    return hashlib.md5(text.encode()).hexdigest()

# üìÑ PowerPoint (.ppt/.pptx) Loader
def load_ppt_file(path: str) -> List[Document]:
    prs = Presentation(path)
    text = ""
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    return [Document(page_content=text, metadata={"source": Path(path).name, "ingested_by": "backend"})]

# üìÅ Load files from folder
def load_new_files(folder: Path, processed: set) -> List[Document]:
    docs = []
    for file in folder.glob("*"):
        ext = file.suffix.lower()
        if ext in SUPPORTED_EXTENSIONS and file.name not in processed:
            try:
                if ext == ".pdf":
                    loader = PyMuPDFLoader(str(file))
                    pages = loader.load()
                elif ext in [".ppt", ".pptx"]:
                    pages = load_ppt_file(str(file))
                else:
                    loader = UnstructuredFileLoader(str(file))
                    pages = loader.load()

                for i, doc in enumerate(pages):
                    doc.metadata["source"] = file.name
                    doc.metadata["page"] = i + 1
                    doc.metadata["ingested_by"] = "backend"
                docs.extend(pages)
                processed.add(file.name)
                logger.info(f"üìÑ Loaded {len(pages)} pages from {file.name}")
            except Exception as e:
                logger.error(f"‚ùå Failed to load {file.name}: {e}")
    return docs

# üåê Web loader
def load_web(urls: List[str], url_cache: dict) -> List[Document]:
    docs = []
    for url in urls:
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.text, "html.parser")
            for tag in soup(["script", "style", "nav", "footer", "header", "noscript"]):
                tag.decompose()
            text = soup.get_text(separator="\n")
            cleaned = "\n".join([line.strip() for line in text.splitlines() if line.strip()])
            hash_val = hash_content(cleaned)
            if url_cache.get(url) == hash_val:
                logger.info(f"üîÑ No change in {url}, skipping...")
                continue
            url_cache[url] = hash_val
            doc = Document(page_content=cleaned, metadata={"source": url, "ingested_by": "backend"})
            docs.append(doc)
        except Exception as e:
            logger.error(f"‚ùå Failed to scrape {url}: {e}")
    return docs

# ‚úÇÔ∏è Chunking
def chunk_documents(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(docs)
    filtered_chunks = []
    for i, chunk in enumerate(chunks):
        if len(chunk.page_content.strip().split()) >= MIN_TOKENS:
            chunk.metadata["chunk_index"] = i
            chunk.metadata["rag_snippet"] = chunk.page_content
            filtered_chunks.append(chunk)
    return filtered_chunks

# üö´ Deduplication
def deduplicate_chunks(chunks: List[Document]) -> List[Document]:
    global indexed_hashes
    unique = []
    for chunk in chunks:
        text = chunk.page_content.strip()
        hash_val = hash_content(text)
        if hash_val not in indexed_hashes:
            indexed_hashes.add(hash_val)
            unique.append(chunk)
    return unique

# üîñ Embed & Save
def update_index(chunks: List[Document], index_path="faiss_backend"):
    """
    Update FAISS index at index_path. Default index_path set to 'faiss_backend'
    so it matches monitoring.py which triggers ingestion with --index-path faiss_backend.
    """
    embedder = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en")
    if Path(index_path).exists():
        index = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)
        index.add_documents(chunks)
    else:
        index = FAISS.from_documents(chunks, embedder)
    index.save_local(index_path)
    logger.info(f"‚úÖ Index updated and saved to '{index_path}'")
    with open(HASH_STORE_PATH, "wb") as f:
        pickle.dump(indexed_hashes, f)

# üß† Main entry point
def run_background_ingestion(pdf_dir: str, urls: List[str], index_path="faiss_backend", benchmark=False):
    """
    Main ingestion routine.
    - pdf_dir: folder path to ingest (this is passed from monitoring.py)
    - urls: optional list of urls to scrape and ingest
    - index_path: FAISS index path (default 'faiss_backend' to match monitoring)
    """
    start = time.time()
    pdf_dir = Path(pdf_dir)
    processed_files = set()
    url_cache = {}

    # Log the target folder and index path so caller (monitoring) can confirm
    logger.info(f"üöÄ Starting ingestion for folder: {pdf_dir} | index: {index_path}")

    # Ensure folder exists
    if not pdf_dir.exists():
        logger.warning(f"‚ö†Ô∏è Provided folder does not exist: {pdf_dir}")
        return

    new_file_docs = load_new_files(pdf_dir, processed_files)
    new_web_docs = load_web(urls, url_cache)

    all_docs = new_file_docs + new_web_docs
    if not all_docs:
        logger.info("üö§ No new documents to process.")
        return

    chunks = chunk_documents(all_docs)
    chunks = deduplicate_chunks(chunks)
    if chunks:
        logger.info(f"‚úÖ {len(chunks)} unique chunks to index.")
        update_index(chunks, index_path)
    else:
        logger.warning("‚ùå No unique chunks after deduplication.")

    if benchmark:
        logger.info(f"‚è±Ô∏è Ingestion completed in {round(time.time() - start, 2)}s")


# ------------------------------
# CLI wrapper so monitoring.py can call this with --folder / --index-path
# ------------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Backend ingestion for FAISS index")
    parser.add_argument("--folder", type=str, required=True, help="Folder containing documents to ingest")
    parser.add_argument("--urls", nargs="*", default=[], help="Optional URLs to ingest alongside files")
    parser.add_argument("--index-path", type=str, default="faiss_backend", help="FAISS index path (default: faiss_backend)")
    parser.add_argument("--benchmark", action="store_true", help="Enable benchmark timing output")
    args = parser.parse_args()

    # Call the ingestion routine with values from CLI (monitoring.py will pass --folder and --index-path)
    run_background_ingestion(
        pdf_dir=args.folder,
        urls=args.urls,
        index_path=args.index_path,
        benchmark=args.benchmark
    )
