{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3ea026",
   "metadata": {},
   "source": [
    "#  Combined Ingestion: PDFs + Web Pages ‚Üí FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4302834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install required packages\n",
    "!pip install langchain sentence-transformers faiss-cpu pymupdf beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97863f8f",
   "metadata": {},
   "source": [
    "## üìÑ Step 1: Load PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ade76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_dir = Path(r\"C:/Rag_data\")\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "all_docs = []\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    loader = PyMuPDFLoader(str(pdf))\n",
    "    pages = loader.load()\n",
    "    for i, doc in enumerate(pages):\n",
    "        doc.metadata[\"source\"] = pdf.name\n",
    "        doc.metadata[\"page\"] = i + 1\n",
    "    all_docs.extend(pages)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(all_docs)} pages from {len(pdf_files)} PDFs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ea822",
   "metadata": {},
   "source": [
    "##  Step 2: Load Web Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a37aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema import Document\n",
    "\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://www.ibm.com/topics/natural-language-processing\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "        cleaned = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "        doc = Document(page_content=cleaned, metadata={\"source\": url})\n",
    "        all_docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to scrape {url}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Total documents after web scraping: {len(all_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454adee8",
   "metadata": {},
   "source": [
    "##  Step 3: Chunk All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc0514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(all_docs)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.metadata[\"chunk_index\"] = i\n",
    "\n",
    "print(f\"üì¶ Created {len(chunks)} chunks.\")\n",
    "print(\"Sample chunk:\", chunks[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4e36f",
   "metadata": {},
   "source": [
    "##  Step 4: Embed and Store in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52362582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "index = FAISS.from_documents(chunks, embedder)\n",
    "index.save_local(\"combined_faiss_index\")\n",
    "\n",
    "print(\"‚úÖ FAISS index saved to 'combined_faiss_index'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81473e",
   "metadata": {},
   "source": [
    "##  Load Embedder and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load embedder\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# Load FAISS index\n",
    "index = FAISS.load_local(\n",
    "    folder_path=\"combined_faiss_index\",\n",
    "    embeddings=embedder,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FAISS index loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61281447",
   "metadata": {},
   "source": [
    "##  Run Semantic Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bca4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a semantic question\n",
    "query = \"What is NLP and how is it used in real life?\"\n",
    "\n",
    "# Get top 3 relevant chunks with similarity scores\n",
    "results = index.similarity_search_with_score(query, k=3)\n",
    "\n",
    "# Display results\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nüîπ Rank {i} (Score: {score:.4f})\")\n",
    "    print(f\"Source: {doc.metadata.get('source')} | Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(doc.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb9cd4",
   "metadata": {},
   "source": [
    "##  Plot the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üßæ Extract scores, sources, and preview text\n",
    "scores = []\n",
    "sources = []\n",
    "texts = []\n",
    "\n",
    "for doc, score in results:\n",
    "    scores.append(score)\n",
    "    sources.append(doc.metadata.get(\"source\", \"unknown\"))\n",
    "    texts.append(doc.page_content[:100].replace(\"\\n\", \" \") + \"...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(range(len(scores)), scores, color='skyblue')\n",
    "plt.yticks(range(len(scores)), [f\"{i+1}. {s}\" for i, s in enumerate(sources)])\n",
    "plt.xlabel(\"Similarity Score (lower is better)\")\n",
    "plt.title(\"Top-k FAISS Document Similarity Scores\")\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_width() + 0.01, bar.get_y() + 0.4, f\"{score:.4f}\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
